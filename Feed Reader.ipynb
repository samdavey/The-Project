{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing an RSS feed\n",
    "This is a work in progress technical POC with the objective of:\n",
    "1. Taking an RSS feed URL\n",
    "2. Reading the items on the feed\n",
    "3. Linking through to the source articles\n",
    "4. Creating a datastructure to hold the atricle and metadata\n",
    "\n",
    "This is intended to be passed to an NL parser that interprets it, a handler that stores the article datastructure and interpretation and a controller that decides whether action should be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before running, use 'pip install feedparser'\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the feedparser to get an RSS feed. This is from my personal Google News\n",
    "feed = 'https://news.google.com/news?cf=all&hl=en&pz=1&ned=au&q=analytics&output=rss'\n",
    "parser = feedparser.parse(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing to understand the feed data structure. Don't need to run.\n",
    "# Check the feed metadata\n",
    "print(parser['feed'])\n",
    "print('\\n')\n",
    "print(parser['feed']['title'])\n",
    "print('\\n')\n",
    "print(parser['feed']['link'])\n",
    "print('\\n')\n",
    "print(parser['feed']['updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Testing to understand the feed data structure. Don't need to run.\n",
    "# How many articles do we have?\n",
    "print(len(parser['entries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing to understand the feed data structure. Don't need to run.\n",
    "# What's in entry 1?\n",
    "#print(parser['entries'][0])\n",
    "print('\\nTitle:\\n' + parser['entries'][0]['title_detail']['value'] + '\\n')\n",
    "print('\\nPublished:\\n' + parser['entries'][0]['published'] + '\\n')\n",
    "print('\\nLink:\\n' + parser['entries'][0]['link'] + '\\n')\n",
    "print('\\nSummary:\\n' + parser['entries'][0]['summary_detail']['value'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The datastructure we want is probably the following:\n",
    "### Article\n",
    "- **id** : a md5 hash of the article URL; only the real bits not the click-through crap\n",
    "- **metadata_feed_name** : name of the RSS feed or search criteria that returned this article\n",
    "- **metadata_feed_link** : URL for the feed that teh article was sourced from\n",
    "- **metadata_feed_accessed** : timestamp when the article was obtained (the first time, duplicates are dropped)\n",
    "- **article_title** : title\n",
    "- **article_published** : timestamp when the article was first published\n",
    "- **article_publisher** : the news source\n",
    "- **article_link** : link to the article (minus the click-through crap)\n",
    "- **article_content** : the text of the article (scrape the HTML and keep only the body content)\n",
    "\n",
    "We then do natural language processing on the Article and produce a new table of article_ids and tuples that represent the themes of the article. E.g. (noun, verb, preposition etc), ('BHP', 'dam', 'collapse'), ('Australian company Lucapa', 'finds', 'huge diamond', 'Angola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_title :  6 Ways to Use Google Analytics You Haven't Thought Of - Search Engine Journal\n",
      "article_publisher :  0\n",
      "metadata_feed_link :  http://news.google.com/news?hl=en&pz=1&ned=au&q=analytics\n",
      "metadata_feed_name :  analytics - Google News\n",
      "article_published :  Tue, 16 Feb 2016 12:07:42 GMT\n",
      "metadata_feed_accessed :  Wed, 17 Feb 2016 13:28:28 GMT\n",
      "article_content :  0\n",
      "id :  5f3852c02d9086f338f535bf397187d3\n",
      "article_link :  http://news.google.com/news/url?sa=t&fd=R&ct2=au&usg=AFQjCNHgv_12F-QA-h7C8wFQYEh4QCwScw&clid=c3a7d30bb8a4878e06b80cf16b898331&ei=e3XEVvD3O6er4ALTl4LIBw&url=https://www.searchenginejournal.com/6-ways-use-google-analytics-havent-thought/155118/\n"
     ]
    }
   ],
   "source": [
    "# Now save the summary of the whole list\n",
    "summary = []\n",
    "\n",
    "for item in parser['entries']:\n",
    "    summary.append({'id': hashlib.md5(item.link.encode('utf-8')).hexdigest(), \n",
    "                    'metadata_feed_name': parser.feed.title, \n",
    "                    'metadata_feed_link': parser.feed.link,\n",
    "                    'metadata_feed_accessed': parser.feed.updated, \n",
    "                    'article_title': item.title, \n",
    "                    'article_published': item.published, \n",
    "                    'article_publisher': 0, # to be parsed and added\n",
    "                    'article_link': item.link,\n",
    "                    'article_content': 0 # to be parsed and added\n",
    "                   })\n",
    "\n",
    "# Print it so we know it worked; remember that dictionaries are not ordered\n",
    "for keys,values in summary[0].items():\n",
    "    print(keys, ': ', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.google.com/news/url?sa=t&fd=R&ct2=au&usg=AFQjCNHgv_12F-QA-h7C8wFQYEh4QCwScw&clid=c3a7d30bb8a4878e06b80cf16b898331&ei=e3XEVvD3O6er4ALTl4LIBw&url=https://www.searchenginejournal.com/6-ways-use-google-analytics-havent-thought/155118/\n",
      "6 Ways to Use Google Analytics You Haven't Thought Of | SEJ\n",
      "\n",
      "Search Marketing Advice, News, and Tutorials\n",
      "Search Marketing Advice, News, and Tutorials\n"
     ]
    }
   ],
   "source": [
    "# testing the link following and html parsing\n",
    "link = summary[0]['article_link']\n",
    "print(link) #works\n",
    "\n",
    "html_doc = requests.get(link).text\n",
    "#print(html_doc) #works\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.find('title').get_text())\n",
    "\n",
    "#for tag in soup.find_all(True):\n",
    "#    print(tag.name)\n",
    "\n",
    "#print('----------------------------------------------------------------')    \n",
    "\n",
    "soup_index = soup.find(\"h1\")\n",
    "print(soup_index.get_text())\n",
    "soup_index_sibling = soup_index.next_sibling\n",
    "print(soup_index_sibling.get_text())\n",
    "soup_index_sibling2 = soup_index_sibling.next_sibling\n",
    "print(soup_index_sibling.get_text())\n",
    "\n",
    "\n",
    "#html = u\"\"\n",
    "#for tag in soup.find(\"h1\").next_siblings:\n",
    "#    if tag.name == \"h1\":\n",
    "#        break\n",
    "#    else:\n",
    "#        print(tag)\n",
    "\n",
    "#print(html)\n",
    "#soup2 = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bugs:\n",
    "- Strip crap out of RSS article links\n",
    "- Parse time stamps into useable formats\n",
    "- Parse the publisher out of the URL or maybe the site name?\n",
    "- Scraping the articles gets super confusing witht eh amount of shit tags. Maybe take everything from the start of the 1st <h1> to the start of the next <h1>. In between there shoudl be the actual data and not the page comments and advertising crap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
